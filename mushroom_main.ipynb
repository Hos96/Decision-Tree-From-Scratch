{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMuqbjkBdTGVJuJSkE29eDA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hos96/Decision-Tree-from-scratch/blob/main/mushroom_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from graphviz import Digraph\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "8UyBcbeURsGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connection to my drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/My Drive/Professor Cesa Bianchi/secondary_data.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghC8it7SQZFi",
        "outputId": "5589a646-a5ea-461f-a132-d7de46ddc71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "3DzmTWPxS8hA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61f7c56-c074-4571-ad67-e35b6e655430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 61069 entries, 0 to 61068\n",
            "Data columns (total 1 columns):\n",
            " #   Column                                                                                                                                                                                                                                             Non-Null Count  Dtype \n",
            "---  ------                                                                                                                                                                                                                                             --------------  ----- \n",
            " 0   class;cap-diameter;cap-shape;cap-surface;cap-color;does-bruise-or-bleed;gill-attachment;gill-spacing;gill-color;stem-height;stem-width;stem-root;stem-surface;stem-color;veil-type;veil-color;has-ring;ring-type;spore-print-color;habitat;season  61069 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 477.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU4AS3ot7v9M"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TreeNode:\n",
        "    def __init__(obj, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        obj.feature = feature\n",
        "        obj.threshold = threshold\n",
        "        obj.left = left\n",
        "        obj.right = right\n",
        "        obj.value = value  # Value like p or e\n",
        "\n",
        "    def is_leaf(obj):\n",
        "        return obj.value is not None\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(obj, maximum_depth=None, max_leaf_nodes=None, entropy_threshold=None, split_function=None, minimum_samples_split=2, feature_names=None):\n",
        "        obj.maximum_depth = maximum_depth\n",
        "        obj.max_leaf_nodes = max_leaf_nodes\n",
        "        obj.entropy_threshold = entropy_threshold\n",
        "        obj.split_function = split_function\n",
        "        obj.minimum_samples_split = minimum_samples_split\n",
        "        obj.root = None\n",
        "        obj.feature_names = feature_names\n",
        "        obj.leaf_count = 0\n",
        "        obj.depth = 0\n",
        "        #criterion_func is a dictionaty to map these names to the corresponding splitting functions\n",
        "        obj.criterion_func = {\n",
        "            'scaled_entropy': obj._scaled_entropy,\n",
        "            'gini': obj._gini_impurity,\n",
        "            'squared': obj._squared_impurity,\n",
        "            #'misclassification': obj._misclassification\n",
        "        }.get(obj.split_function)\n",
        "\n",
        "    def get_params(obj, deep=True):\n",
        "        return {\n",
        "            'maximum_depth': obj.maximum_depth,\n",
        "            'max_leaf_nodes': obj.max_leaf_nodes,\n",
        "            'entropy_threshold': obj.entropy_threshold,\n",
        "            'split_function': obj.split_function,\n",
        "            'minimum_samples_split': obj.minimum_samples_split,\n",
        "            'feature_names': obj.feature_names\n",
        "        }\n",
        "\n",
        "    def set_params(obj, **params):# params is a dictionary\n",
        "        for param, value in params.items():\n",
        "            setattr(obj, param, value)# for modifing the hyper parameters of tree\n",
        "        return obj\n",
        "\n",
        "    def _grow_tree(obj, X, y, current_depth=0):\n",
        "        #X numpy array 2 dimensional, y = one dimension numpy array, current_depth at the begining is zero\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        current_entropy = obj.criterion_func(y)\n",
        "\n",
        "        if current_depth > obj.depth: # for updating the depth\n",
        "            obj.depth = current_depth\n",
        "\n",
        "        if (current_depth >= obj.maximum_depth) \\ #checks the below lines for stopping the tree\n",
        "                or (obj.leaf_count >= obj.max_leaf_nodes) \\\n",
        "                or (obj.entropy_threshold is not None and current_entropy < obj.entropy_threshold) \\\n",
        "                or (num_samples < obj.minimum_samples_split) \\\n",
        "                or (np.unique(y).size == 1): #same class check\n",
        "            leaf_value = obj._most_common_label(y) #assigning the class lable\n",
        "            return TreeNode(value=leaf_value)\n",
        "\n",
        "        feature_indexes = np.random.choice(num_features, num_features, replace=False)# randomness (needs to change!!)\n",
        "        best_feat, best_thresh = obj._best_feature_to_split_based_on(X, y, feature_indexes)\n",
        "        if best_feat is None:\n",
        "            leaf_value = obj._most_common_label(y)\n",
        "            return TreeNode(value=leaf_value)\n",
        "\n",
        "        obj.leaf_count += 1\n",
        "        left_idxs, right_idxs = obj._split(X[:, best_feat], best_thresh)\n",
        "        left = obj._grow_tree(X[left_idxs, :], y[left_idxs], current_depth + 1)\n",
        "        right = obj._grow_tree(X[right_idxs, :], y[right_idxs], current_depth + 1)\n",
        "        return TreeNode(best_feat, best_thresh, left, right)\n",
        "\n",
        "    def fit(obj, X, y):\n",
        "        obj.root = obj._grow_tree(X, y)\n",
        "\n",
        "    def _best_feature_to_split_based_on(obj, X, y, feature_indexes):\n",
        "        best_information_gain = -1\n",
        "        split_idx, split_thresh = None, None #split_idx: index of the best feature\n",
        "        for index_counter in feature_indexes:# search for all indexes in the list of indexes\n",
        "            X_column = X[:, index_counter]\n",
        "            thresholds = np.unique(X_column)# split_thresh the optimal thresh for the best feature\n",
        "            for threshold in thresholds:\n",
        "                gain = obj._information_gain(y, X_column, threshold)\n",
        "                if gain > best_information_gain:\n",
        "                    best_information_gain = gain\n",
        "                    split_idx = index_counter\n",
        "                    split_thresh = threshold\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _split(obj, X_column, split_thresh): # returns two lists of left indexes(lower than thresh) and right (higher than threrhs)\n",
        "        #X_column is a list containing all the values of the feature column\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _information_gain(obj, y, X_column, split_thresh):\n",
        "\n",
        "        parent_criterion = obj.criterion_func(y)\n",
        "\n",
        "        left_idxs, right_idxs = obj._split(X_column, split_thresh)\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        y_left, y_right = y[left_idxs], y[right_idxs]\n",
        "        child_criterion = obj._weighted_criterion(y_left, y_right, obj.criterion_func)\n",
        "        gain = parent_criterion - child_criterion\n",
        "        return gain\n",
        "\n",
        "    def _weighted_criterion(obj, y_left, y_right, criterion_func): #Entropy/ gini average of left and right\n",
        "        n = len(y_left) + len(y_right)\n",
        "        p_left = len(y_left) / n\n",
        "        p_right = len(y_right) / n\n",
        "        return p_left * criterion_func(y_left) + p_right * criterion_func(y_right)\n",
        "\n",
        "    # From class lectures scaled_ent = - (p/2)*np.log2(p) - ((1-p)/2)*np.log2(1-p) for binary classification\n",
        "    def _scaled_entropy(obj, y):\n",
        "        hist = np.bincount(y)\n",
        "        probs = hist / len(y)\n",
        "        scaled_ent = -np.sum([(p / 2) * np.log2(p) for p in probs if p > 0])\n",
        "        return scaled_ent\n",
        "\n",
        "    #From class lectures it should be 2p(1-p) for binary classification, but let use the general formula for non-binary case\n",
        "    def _gini_impurity(obj, y):\n",
        "        hist = np.bincount(y)\n",
        "        probs = hist / len(y)\n",
        "        gini = 1.0 - np.sum(probs ** 2)  # gini = np.sum(probs * (1 - probs))\n",
        "        return gini\n",
        "\n",
        "    # for binary classification    sqrt(p*(1-p))\n",
        "    def _squared_impurity(obj, y):\n",
        "        hist = np.bincount(y)\n",
        "        probs = hist / len(y)\n",
        "        epsilon = 1e-10  # Small constant to avoid multiplying by zero\n",
        "        sqr = np.sum(np.sqrt((probs + epsilon) * (1 - probs + epsilon)))\n",
        "        return sqr\n",
        "\n",
        "    #def _misclassification(obj, y):\n",
        "    #    hist = np.bincount(y)\n",
        "    #    probs = hist / len(y)\n",
        "    #    mce = 1.0 - np.max(probs)\n",
        "    #    return mce\n",
        "\n",
        "    def _most_common_label(obj, y):\n",
        "        return np.bincount(y).argmax()\n",
        "\n",
        "    def predict(obj, X):\n",
        "        return np.array([obj.navigation_tree_for_prediction(x, obj.root) for x in X])\n",
        "\n",
        "    def navigation_tree_for_prediction(obj, x, node):# recursive function\n",
        "        if node.is_leaf():\n",
        "            return node.value\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return obj.navigation_tree_for_prediction(x, node.left)\n",
        "        return obj.navigation_tree_for_prediction(x, node.right)\n",
        "\n",
        "    def visualize_tree(obj, dot=None):\n",
        "        if dot is None:\n",
        "            dot = Digraph()\n",
        "\n",
        "        # Recursive function to add nodes and edges\n",
        "        def add_nodes_edges(dot, node): # for graphical design\n",
        "            if node is None:\n",
        "                return\n",
        "            if node.is_leaf():\n",
        "                dot.node(str(id(node)), f\"Class {node.value}\", shape='ellipse', style='filled', fillcolor='lightgreen')\n",
        "            else:\n",
        "                if obj.feature_names is not None:\n",
        "                    feature_name = obj.feature_names[node.feature]\n",
        "                else:\n",
        "                    feature_name = f\"Feature {node.feature}\"\n",
        "                dot.node(str(id(node)), f\"{feature_name} <= {node.threshold:.2f}\", shape='box', style='filled', fillcolor='lightblue')\n",
        "                if node.left is not None:\n",
        "                    add_nodes_edges(dot, node.left)\n",
        "                    dot.edge(str(id(node)), str(id(node.left)), '<=', color='blue')\n",
        "                if node.right is not None:\n",
        "                    add_nodes_edges(dot, node.right)\n",
        "                    dot.edge(str(id(node)), str(id(node.right)), '>', color='red')\n",
        "\n",
        "        add_nodes_edges(dot, obj.root)\n",
        "        return dot\n",
        "\n",
        "def zero_one_loss(y_actual, y_training_prediction):\n",
        "    return np.mean(y_training_prediction != y_actual)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load the dataset\n",
        "    data = df\n",
        "\n",
        "    print(data.head())\n",
        "\n",
        "    # one-hot encoding\n",
        "    data_encoded = pd.get_dummies(data)\n",
        "    data_encoded = data_encoded.astype(int)\n",
        "    print(data_encoded.head())\n",
        "\n",
        "    # features vs target\n",
        "    X = data_encoded.drop(['class_p', 'class_e'], axis=1)  # Assuming 'class_p' is the target\n",
        "    y = data_encoded['class_p']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "\n",
        "    tree = DecisionTree(\n",
        "        maximum_depth=None,\n",
        "        max_leaf_nodes=None,\n",
        "        entropy_threshold=0.2,\n",
        "        split_function=\"gini\",\n",
        "        minimum_samples_split=2,\n",
        "        feature_names=X_train.columns\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    tree.fit(X_train.values, y_train.values)\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    print(f\"\\nExecution time: {execution_time} seconds\")\n",
        "\n",
        "    # predictions on the training data\n",
        "    y_training_prediction = tree.predict(X_train.values)\n",
        "\n",
        "    # model on the training data\n",
        "    accuracy = accuracy_score(y_train, y_training_prediction)\n",
        "    print(f\"\\nTraining accuracy: {accuracy:.6f}\")\n",
        "\n",
        "    # Predict on the testing data\n",
        "    y_test_pred = tree.predict(X_test.values)\n",
        "\n",
        "    # Evaluate the model on the testing data\n",
        "    accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"Test accuracy: {accuracy:.6f}\")\n",
        "\n",
        "    train_error = zero_one_loss(y_test.values, y_test_pred)# the zero-one loss\n",
        "    print(f\"zero one loss on test set: {train_error:.6f}\")\n",
        "\n",
        "    #confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test.values, y_test_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Red', xticklabels=['Class 0', 'Class 1'],# show the confusion matrix\n",
        "                yticklabels=['Class 0', 'Class 1'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    # Visualize the tree\n",
        "    dot = Digraph()\n",
        "    dot = tree.visualize_tree(dot)\n",
        "    dot.render('png/mushroom_tree', format='png', view=True)"
      ]
    }
  ]
}