{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmabrllxyBzq+70f4PdMbj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hos96/Decision-Tree-from-scratch/blob/main/parameter%20tuning%20code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install graphviz"
      ],
      "metadata": {
        "id": "6tt_-TOYJC2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import make_scorer, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from graphviz import Digraph\n",
        "from joblib import Parallel, delayed"
      ],
      "metadata": {
        "id": "O5HrXasUKWRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TreeNode:\n",
        "    def __init__(obj, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        obj.feature = feature\n",
        "        obj.threshold = threshold\n",
        "        obj.left = left\n",
        "        obj.right = right\n",
        "        obj.value = value  # Value the node has if it is a leaf\n",
        "\n",
        "    def is_leaf(obj):\n",
        "        return obj.value is not None"
      ],
      "metadata": {
        "id": "9L9BF_LLKa-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj5gvht_76xr"
      },
      "outputs": [],
      "source": [
        "class DecisionTree:\n",
        "    def __init__(obj, maximum_depth=None, max_leaf_nodes=None, entropy_threshold=None, spliting_function=None, min_samples_split=2, feature_names=None):\n",
        "        obj.maximum_depth = maximum_depth\n",
        "        obj.max_leaf_nodes = max_leaf_nodes\n",
        "        obj.entropy_threshold = entropy_threshold\n",
        "        obj.spliting_function = spliting_function\n",
        "        obj.min_samples_split = min_samples_split\n",
        "        obj.root = None\n",
        "        obj.feature_names = feature_names\n",
        "        obj.leaf_count = 0\n",
        "        obj.depth = 0\n",
        "        obj.criterion_func = {\n",
        "            'scaled_entropy': obj._scaled_entropy,\n",
        "            'gini': obj._gini_impurity,\n",
        "            'squared': obj._squared_impurity,\n",
        "        }.get(obj.spliting_function)\n",
        "\n",
        "    def get_parameters(obj, deep=True):\n",
        "        return {\n",
        "            'maximum_depth': obj.maximum_depth,\n",
        "            'max_leaf_nodes': obj.max_leaf_nodes,\n",
        "            'entropy_threshold': obj.entropy_threshold,\n",
        "            'spliting_function': obj.spliting_function,\n",
        "            'min_samples_split': obj.min_samples_split,\n",
        "            'feature_names': obj.feature_names\n",
        "        }\n",
        "\n",
        "    def set_parameters(obj, **params):#for hyper parameter tuning\n",
        "        for param, value in params.items():\n",
        "            setattr(obj, param, value)#a function in python for modifing the attributes of objects\n",
        "        return obj\n",
        "\n",
        "    def fit(obj, X, y):\n",
        "        obj.root = obj._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(obj, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        current_entropy = obj.criterion_func(y)\n",
        "\n",
        "        if depth > obj.depth:\n",
        "            obj.depth = depth\n",
        "\n",
        "        if (obj.maximum_depth is not None and depth >= obj.maximum_depth) \\\n",
        "                or (obj.max_leaf_nodes is not None and obj.leaf_count >= obj.max_leaf_nodes) \\\n",
        "                or (obj.entropy_threshold is not None and current_entropy < obj.entropy_threshold) \\\n",
        "                or (num_samples < obj.min_samples_split) \\\n",
        "                or (np.unique(y).size == 1):\n",
        "            leaf_value = obj._most_common_label(y)\n",
        "            return TreeNode(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(num_features, num_features, replace=False)\n",
        "        best_feat, best_thresh = obj._best_criteria(X, y, feat_idxs)\n",
        "        if best_feat is None:\n",
        "            leaf_value = obj._most_common_label(y)\n",
        "            return TreeNode(value=leaf_value)\n",
        "\n",
        "        obj.leaf_count += 1\n",
        "        left_idxs, right_idxs = obj._split(X[:, best_feat], best_thresh)\n",
        "        left = obj._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "        right = obj._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "        return TreeNode(best_feat, best_thresh, left, right)\n",
        "\n",
        "    def _best_criteria(obj, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_thresh = None, None\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "            for threshold in thresholds:\n",
        "                gain = obj._gain(y, X_column, threshold)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_thresh = threshold\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _split(obj, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _gain(obj, y, X_column, split_thresh):\n",
        "\n",
        "        parent_criterion = obj.criterion_func(y)\n",
        "\n",
        "        left_idxs, right_idxs = obj._split(X_column, split_thresh)\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        y_left, y_right = y[left_idxs], y[right_idxs]\n",
        "        child_criterion = obj._weighted_criterion(y_left, y_right, obj.criterion_func)\n",
        "        gain = parent_criterion - child_criterion\n",
        "        return gain\n",
        "\n",
        "    def _weighted_criterion(obj, y_left, y_right, criterion_func):\n",
        "        n = len(y_left) + len(y_right)\n",
        "        p_left = len(y_left) / n\n",
        "        p_right = len(y_right) / n\n",
        "        return p_left * criterion_func(y_left) + p_right * criterion_func(y_right)\n",
        "\n",
        "    def _scaled_entropy(obj, y):\n",
        "        hist = np.bincount(y)\n",
        "        probs = hist / len(y)\n",
        "        scaled_ent = -np.sum([(p / 2) * np.log2(p) for p in probs if p > 0])\n",
        "        return scaled_ent\n",
        "\n",
        "    def _gini_impurity(obj, y):\n",
        "        hist = np.bincount(y)\n",
        "        probs = hist / len(y)\n",
        "        gini = 1.0 - np.sum(probs ** 2)\n",
        "        return gini\n",
        "    def _squared_impurity(obj, y):\n",
        "        hist = np.bincount(y)\n",
        "        probs = hist / len(y)\n",
        "        epsilon = 1e-10  #\n",
        "        sqr = np.sum(np.sqrt((probs + epsilon) * (1 - probs + epsilon)))\n",
        "        return sqr\n",
        "\n",
        "    def predict(obj, X):\n",
        "        return np.array([obj._investigate_tree(x, obj.root) for x in X])\n",
        "\n",
        "    def _most_common_label(obj, y):\n",
        "        return np.bincount(y).argmax()\n",
        "\n",
        "    def _investigate_tree(obj, x, node):\n",
        "        if node.is_leaf():\n",
        "            return node.value\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return obj._investigate_tree(x, node.left)\n",
        "        return obj._investigate_tree(x, node.right)\n",
        "\n",
        "    def visualize_tree(obj, dot=None):\n",
        "        if dot is None:\n",
        "            dot = Digraph()\n",
        "\n",
        "        def add_nodes_edges(dot, node):\n",
        "            if node is None:\n",
        "                return\n",
        "            if node.is_leaf():\n",
        "                dot.node(str(id(node)), f\"Class {node.value}\", shape='ellipse')\n",
        "            else:\n",
        "                if obj.feature_names is not None:\n",
        "                    feature_name = obj.feature_names[node.feature]\n",
        "                else:\n",
        "                    feature_name = f\"Feature {node.feature}\"\n",
        "                dot.node(str(id(node)), f\"{feature_name} <= {node.threshold}\", shape='box')\n",
        "                if node.left is not None:\n",
        "                    add_nodes_edges(dot, node.left)\n",
        "                    dot.edge(str(id(node)), str(id(node.left)), '<=')\n",
        "                if node.right is not None:\n",
        "                    add_nodes_edges(dot, node.right)\n",
        "                    dot.edge(str(id(node)), str(id(node.right)), '>')\n",
        "\n",
        "        add_nodes_edges(dot, obj.root)\n",
        "        return dot\n",
        "\n",
        "def zero_one_loss(y_true, y_pred):\n",
        "    return np.mean(y_pred != y_true)\n",
        "\n",
        "def grid_search(X_train, y_train, param_grid, scoring_func):\n",
        "\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    def evaluate_parameters(params):\n",
        "        current_scores = []\n",
        "        depths = []\n",
        "        leafs = []\n",
        "\n",
        "        # 80% train, 20% validation\n",
        "        for train_idxs, val_idxs in cv.split(X_train, y_train):\n",
        "            X_train_cv, y_train_cv = X_train.iloc[train_idxs], y_train.iloc[train_idxs]\n",
        "            X_val_cv, y_val_cv = X_train.iloc[val_idxs], y_train.iloc[val_idxs]\n",
        "\n",
        "            model = DecisionTree(\n",
        "                maximum_depth=params.get('maximum_depth'),\n",
        "                max_leaf_nodes=params.get('max_leaf_nodes'),\n",
        "                entropy_threshold=params.get('entropy_threshold'),\n",
        "                spliting_function=params['spliting_function'],\n",
        "                min_samples_split=2,\n",
        "                feature_names=X_train.columns\n",
        "            )\n",
        "\n",
        "            model.fit(X_train_cv.values, y_train_cv.values)\n",
        "            y_val_pred = model.predict(X_val_cv.values)\n",
        "            score = scoring_func(y_val_cv, y_val_pred)\n",
        "            #print(f\"zero one loss: {score:.6f} with params: {params}\")\n",
        "            current_scores.append(score)\n",
        "            depths.append(model.depth)\n",
        "            leafs.append(model.leaf_count)\n",
        "\n",
        "\n",
        "        mean_score = np.mean(current_scores)\n",
        "        mean_depth = np.mean(depths)\n",
        "        mean_leafs = np.mean(leafs)\n",
        "\n",
        "        print(f\"zero one loss: {mean_score:.5f} with params: {params} \\t mean depth: {mean_depth:.1f} and mean leafs: {mean_leafs:.1f}\")\n",
        "\n",
        "        return params, mean_score\n",
        "\n",
        "    param_combinations = [\n",
        "        dict(zip(param_dict.keys(), values)) for param_dict in param_grid for values in\n",
        "        itertools.product(*param_dict.values())\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nTotal number of combinations: {len(param_combinations)}  x  5 cv = {5*len(param_combinations)} iterations\\n\")\n",
        "\n",
        "    # Parallelizing the grid search\n",
        "    results = Parallel(n_jobs=-1)(delayed(evaluate_parameters)(params) for params in param_combinations)\n",
        "\n",
        "    # Find best parameters based on the returned results\n",
        "    sorted_results = sorted(results, key=lambda x: x[1])[:10]\n",
        "\n",
        "    print(\"\\nTop 10 Results:\")\n",
        "    for rank, (params, mean_score) in enumerate(sorted_results, 1):\n",
        "        print(f\"Rank {rank}: Mean zero one loss: {mean_score:.6f} with params: {params}\")\n",
        "\n",
        "    return results, sorted_results[0][0], sorted_results[0][1]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load the dataset\n",
        "    data = df\n",
        "\n",
        "    data_encoded = pd.get_dummies(data)\n",
        "    data_encoded = data_encoded.astype(int)\n",
        "\n",
        "\n",
        "    # Separate features and target\n",
        "    X = data_encoded.drop(['class_p', 'class_e'], axis=1)  # Assuming 'class_p' is the target\n",
        "    y = data_encoded['class_p']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "    param_grid = [\n",
        "        {\n",
        "            'maximum_depth': [40],\n",
        "            'spliting_function': ['scaled_entropy', 'gini', 'squared']\n",
        "        },\n",
        "        {\n",
        "            'max_leaf_nodes': [150],\n",
        "            'spliting_function': ['scaled_entropy', 'gini', 'squared']\n",
        "        },\n",
        "        {\n",
        "            'entropy_threshold': [0.0001],\n",
        "            'spliting_function': ['scaled_entropy', 'gini', 'squared']\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    custom_scorer = make_scorer(zero_one_loss, greater_is_better=False)\n",
        "\n",
        "    start_time = time.time()\n",
        "    results, best_params, best_score = grid_search(X_train, y_train, param_grid, zero_one_loss)\n",
        "    end_time = time.time()\n",
        "\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution time: {execution_time} seconds\")\n",
        "\n",
        "    print(f\"\\nBest score: : {best_score:.6f}\")\n",
        "    print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "\n",
        "    best_tree = DecisionTree(\n",
        "        maximum_depth=best_params.get('maximum_depth'),\n",
        "        max_leaf_nodes=best_params.get('max_leaf_nodes'),\n",
        "        entropy_threshold=best_params.get('entropy_threshold'),\n",
        "        spliting_function=best_params['spliting_function'],\n",
        "        min_samples_split=2,  # This is fixed as per the original configuration\n",
        "        feature_names=X_train.columns\n",
        "    )\n",
        "\n",
        "\n",
        "    best_tree.fit(X_train.values, y_train.values)\n",
        "\n",
        "    # predictions on the training data\n",
        "    y_pred = best_tree.predict(X_train.values)\n",
        "\n",
        "    # Evaluate the model for training data\n",
        "    accuracy = accuracy_score(y_train, y_pred)\n",
        "    print(f\"\\nTrain accuracy: {accuracy:.6f}\")\n",
        "\n",
        "    y_test_pred = best_tree.predict(X_test.values)    # Prediction on the testing data\n",
        "\n",
        "\n",
        "    # Evaluate the model on the testing data\n",
        "    accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"Test accuracy: {accuracy:.6f}\")\n",
        "\n",
        "    # zero-one loss\n",
        "    train_error = zero_one_loss(y_test.values, y_test_pred)\n",
        "    print(f\"zero one loss on test set with best params: {train_error:.6f}\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test.values, y_test_pred)  # confusion matrix\n",
        "\n",
        "\n",
        "    # confusion matrix\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds', xticklabels=['Class 0', 'Class 1'],\n",
        "                yticklabels=['Class 0', 'Class 1'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    # Visualize the tree\n",
        "    dot = Digraph()\n",
        "    dot = best_tree.visualize_tree(dot)\n",
        "    dot.render('png/mushroom_tree', format='png', view=True)\n"
      ]
    }
  ]
}